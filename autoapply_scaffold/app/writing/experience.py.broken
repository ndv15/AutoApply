"""
Enhanced experience bullet generation with A-M-O-T format and MMR ranking.
All bullets default to PENDING status and follow Action-Metric-Outcome-Tool format.
"""
import re
import uuid
import json
import hashlib
from pathlib import Path
from collections import Counter
from typing import Dict, List, Optional
from .suggestion_store import SuggestionStore
from .researcher import gather_research_context
from .bullet_debate import run_debate
from .judge import judge_bullets
from .tone import polish_tone
from .scorer import score_bullet
from ..llm.router import LLMRouter
from ..research.cache import get_cached_research, save_cached_research
from ..ranking.providers.cohere import score_bullets, compute_mmr


def extract_jd_themes(jd: str, top_n: int = 5) -> list:
    """Extract key themes from job description."""
    stopwords = {
        'the', 'and', 'or', 'is', 'are', 'was', 'were', 'will', 'be', 'to', 'of', 'in', 'for',
        'on', 'with', 'at', 'by', 'from', 'as', 'an', 'a', 'this', 'that'
    }
    words = re.findall(r'\b[a-zA-Z]{4,}\b', jd.lower())
    filtered = [w for w in words if w not in stopwords]
    counter = Counter(filtered)
    return [word for word, count in counter.most_common(top_n)]


def extract_metrics(text: str) -> List[str]:
    """Extract numeric metrics from text."""
    patterns = [
        r'\$[\d,]+(?:\s*-\s*\$[\d,]+)?',  # Currency ranges
        r'\d+(?:\.\d+)?%(?:\s*-\s*\d+(?:\.\d+)?%)?',  # Percentage ranges
        r'\d+(?:\s*-\s*\d+)?(?=\s+(?:users|customers|clients|deals|sales|leads))',  # Count ranges
    ]
    metrics = []
    for pattern in patterns:
        metrics.extend(re.findall(pattern, text))
    return metrics

def has_amot_format(text: str) -> bool:
    """Check if text follows Action-Metric-Outcome-Tool format."""
    # Action: Starts with strong verb
    if not re.match(r'^(?:Led|Drove|Increased|Implemented|Developed|Created|Built|Managed|Achieved|Generated)', text):
        return False
        
    # Metric: Contains at least one numeric value
    if not extract_metrics(text):
        return False
        
    # Outcome: Contains business impact words
    impact_words = ['resulting', 'leading to', 'achieving', 'generating', 'improving', 'reducing']
    if not any(word in text.lower() for word in impact_words):
        return False
        
    # Tool/Context: Contains specific platform/tool/method
    tools_pattern = r'using|via|through|leveraging|with'
    if not re.search(tools_pattern, text):
        return False
        
    return True

def inject_metrics(text: str) -> str:
    """Insert placeholder metrics if none exist."""
    if extract_metrics(text):
        return text
        
    # Common metric patterns to inject
    patterns = [
        (r'(increased|improved|enhanced|grew)\s+(\w+)', r'\1 \2 by X-Y%'),
        (r'(generated|created|produced)\s+(\w+)', r'\1 $A-$B in \2'),
        (r'(managed|led|supervised)\s+team', r'\1 team of X-Y members'),
    ]
    
    for pattern, replacement in patterns:
        if re.search(pattern, text, re.IGNORECASE):
            return re.sub(pattern, replacement, text, flags=re.IGNORECASE)
            
    return text

def generate_next_suggestion(job_id: str, role_key: str, jd_text: str, role_context: Dict) -> Optional[Dict]:
    """Generate next suggestion for review."""
    store = SuggestionStore(job_id)
    router = LLMRouter()
    
    # Check cache for research
    research = get_cached_research(jd_text)
    if not research:
        research = router.research_role(jd_text, role_context["title"])
        save_cached_research(jd_text, research)
    
    # Generate initial candidates (15-30)
    candidates = []
    for _ in range(3):  # Try up to 3 rounds
        new_candidates = router.generate_suggestions(
            role_context=role_context,
            jd_text=jd_text,
            research_context=research,
            n=10
        )
        
        # Enhance with metrics if needed
        for c in new_candidates:
            if not has_amot_format(c["text"]):
                c["text"] = inject_metrics(c["text"])
            candidates.append(c)
            
        if len(candidates) >= 15:
            break
            
    if not candidates:
        return None
        
    # Score and rank candidates
    scored_candidates = score_bullets(candidates, jd_text)
    
    # Apply MMR to get diverse suggestions
    mmr_candidates = compute_mmr(
        scored_candidates,
        lambda_param=0.7,
        sim_threshold=0.90
    )
    
    # Take the top candidate
    if mmr_candidates:
        suggestion = mmr_candidates[0]
        suggestion_id = store.add_suggestion(role_key, suggestion)
        suggestion['id'] = suggestion_id
        return suggestion
        
    return None

def get_role_quota(role_key: str) -> int:
    """Get the target number of bullets for a role."""
    quotas = {
        'ccs': 4,
        'brightspeed_ii': 4,
        'brightspeed_i': 4,
        'virsatel': 3
    }
    return quotas.get(role_key, 3)  # Default to 3

def propose_experience_bullets(history, jd):
    """
    Generate experience bullets for each role in employment history.

    All bullets start with status='PENDING'. User must explicitly approve.
    Bullets with inferred metrics are marked synth_metric=True.

    Args:
        history: List of employment history dicts with role_id, company, title
        jd: Job description text

    Returns:
        dict: {role_id: [bullet_dict, ...]}
              Each bullet_dict has: id, text, synth_metric, status, confidence
    """
    themes = extract_jd_themes(jd, top_n=5)
    theme_keywords = themes[:3]

    results = {}
    for role in history:
        role_id = role['role_id']
        company = role.get('company', '')
        title = role.get('title', '')
        
        # Role-specific bullet templates based on job title
        if 'Sales Manager' in title:
            bullets = [
                {
                    'id': f'{role_id}-b1',
                    'text': f'Led team of 8 Sales Representatives achieving 115% of team quota through strategic territory planning and deal coaching',
                    'synth_metric': True,
                    'status': 'PENDING',
                    'confidence': 0.92
                },
                {
                    'id': f'{role_id}-b2',
                    'text': f'Implemented enhanced MEDDIC qualification process improving win rates by 22% and reducing sales cycle by 45 days',
                    'synth_metric': True,
                    'status': 'PENDING',
                    'confidence': 0.89
                },
                {
                    'id': f'{role_id}-b3',
                    'text': f'Established new partner channel program generating $2.5M in incremental pipeline within first 6 months',
                    'synth_metric': True,
                    'status': 'PENDING',
                    'confidence': 0.87
                }
            ]
        elif 'Enterprise Account Manager' in title:
            bullets = [
                {
                    'id': f'{role_id}-b1',
                    'text': f'Exceeded annual quota 125% managing $12M territory through strategic account planning and C-level relationships',
                    'synth_metric': True,
                    'status': 'PENDING',
                    'confidence': 0.94
                },
                {
                    'id': f'{role_id}-b2',
                    'text': f'Closed largest deal in company history ($2.8M) through multi-threaded approach and executive sponsorship',
                    'synth_metric': True,
                    'status': 'PENDING',
                    'confidence': 0.91
                },
                {
                    'id': f'{role_id}-b3',
                    'text': f'Built $8M+ pipeline from outbound prospecting leveraging industry insights and personalized account strategies',
                    'synth_metric': True,
                    'status': 'PENDING',
                    'confidence': 0.88
                }
            ]
            if 'II' in title:  # Additional bullet for senior level
                bullets.append({
                    'id': f'{role_id}-b4',
                    'text': f'Mentored 4 new Account Executives achieving 85%+ quota attainment through deal strategy and MEDDIC coaching',
                    'synth_metric': True,
                    'status': 'PENDING',
                    'confidence': 0.86
                })
        else:  # Account Executive
            bullets = [
                {
                    'id': f'{role_id}-b1',
                    'text': f'Ranked #1 Account Executive achieving 147% of quota ($3.2M) through consultative enterprise sales approach',
                    'synth_metric': True,
                    'status': 'PENDING',
                    'confidence': 0.93
                },
                {
                    'id': f'{role_id}-b2',
                    'text': f'Generated 60% of opportunities through strategic prospecting and industry-focused outbound campaigns',
                    'synth_metric': True,
                    'status': 'PENDING',
                    'confidence': 0.89
                },
                {
                    'id': f'{role_id}-b3',
                    'text': f'Maintained 92% customer retention rate through quarterly business reviews and proactive account management',
                    'synth_metric': True,
                    'status': 'PENDING',
                    'confidence': 0.87
                }
            ]

        # Add awards if present
        if role.get('awards'):
            award_text = ' | '.join(role['awards'])
            bullets.insert(0, {
                'id': f'{role_id}-awards',
                'text': f'ðŸ† {award_text}',
                'synth_metric': False,
                'status': 'PENDING',
                'confidence': 1.0
            })

        results[role_id] = bullets

    return results


def _get_suggestion_history_path(job_id: str, role_id: str) -> Path:
    """Get path to suggestion history file."""
    history_dir = Path("out/suggestion_history")
    history_dir.mkdir(parents=True, exist_ok=True)
    return history_dir / f"{job_id}_{role_id}.json"


def _load_suggestion_history(job_id: str, role_id: str) -> set:
    """Load set of previously generated suggestion texts."""
    history_path = _get_suggestion_history_path(job_id, role_id)
    if history_path.exists():
        data = json.loads(history_path.read_text())
        return set(data.get('suggestions', []))
    return set()


def _save_suggestion(job_id: str, role_id: str, suggestion_text: str):
    """Save a new suggestion to history."""
    history_path = _get_suggestion_history_path(job_id, role_id)
    history = _load_suggestion_history(job_id, role_id)
    history.add(suggestion_text)
    history_path.write_text(json.dumps({
        'suggestions': list(history)
    }, indent=2))


def generate_suggestions(
    job_id: str,
    role_key: str,
    jd_text: str,
    role_context: Dict,
    existing_bullets: list = None,
    config: Dict = None,
    n: int = 3
) -> List[Dict]:
    """
    Multi-LLM pipeline to generate N bullet suggestions using LLMRouter.

    Pipeline: perplexity â†’ claude â†’ gpt â†’ judge â†’ gemini? â†’ cohere

    Args:
        job_id: Job ID for tracking
        role_key: Role identifier (e.g., 'ccs-2025')
        jd_text: Full job description text
        role_context: Dict with role_id, company, title, start, end, location
        existing_bullets: List of already approved bullets for this role
        config: Full config dict from config.yaml
        n: Number of suggestions to generate (default 3)

    Returns:
        List[Dict]: [
            {
                "id": str,
                "text": str,
                "score_1_10": int,
                "model_used": str,
                "citations": List[str],
                "source": {...}
            },
            ...
        ]
    """
    if config is None:
        from ..config import load_settings
        config = load_settings().dict()

    try:
        # Initialize LLMRouter (loads config from environment)
        router = LLMRouter()

        # Load suggestion history to avoid repeats
        history = _load_suggestion_history(job_id, role_key)

        # Generate suggestions using router
        suggestions = router.generate_suggestions(
            role_context=role_context,
            jd_text=jd_text,
            research_context=None,  # Router handles research internally
            n=n,
            use_perplexity=config.get('research', {}).get('perplexity', {}).get('enabled', True)
        )

        # Filter out any duplicates from history
        filtered_suggestions = []
        for sugg in suggestions:
            if sugg['text'] not in history:
                filtered_suggestions.append(sugg)
                _save_suggestion(job_id, role_key, sugg['text'])

        # If we filtered too many, generate more
        while len(filtered_suggestions) < n and len(filtered_suggestions) < len(suggestions) * 2:
            additional = router.generate_suggestions(
                role_context=role_context,
                jd_text=jd_text,
                research_context=None,
                n=1,
                use_perplexity=False  # Skip research on retry
            )
            for sugg in additional:
                if sugg['text'] not in history:
                    filtered_suggestions.append(sugg)
                    _save_suggestion(job_id, role_key, sugg['text'])
                    if len(filtered_suggestions) >= n:
                        break

        return filtered_suggestions[:n]

    except Exception as e:
        print(f"Error in generate_suggestions: {e}")
        import traceback
        traceback.print_exc()

        # Fallback suggestions
        fallback = []
        for i in range(n):
            fallback.append({
                "id": str(uuid.uuid4())[:8],
                "text": "Achieved significant results through strategic planning and execution",
                "score_1_10": 5,
                "model_used": "fallback",
                "citations": [],
                "source": {
                    "error": str(e)[:100]
                }
            })
        return fallback


def generate_suggestion(
    job_id: str,
    role_key: str,
    jd_text: str,
    role_context: Dict,
    existing_bullets: list = None,
    fast_model: str = "gpt-3.5-turbo",
    premium_model: str = "claude-3-5-sonnet-20241022",
    config: Dict = None
) -> Dict:
    """
    Legacy single-suggestion generator for backwards compatibility.
    Now wraps generate_suggestions() and returns first result.
    """
    suggestions = generate_suggestions(
        job_id=job_id,
        role_key=role_key,
        jd_text=jd_text,
        role_context=role_context,
        existing_bullets=existing_bullets,
        config=config,
        n=1
    )
    return suggestions[0] if suggestions else {
        "id": str(uuid.uuid4())[:8],
        "text": "Achieved significant results through strategic planning and execution",
        "score_1_10": 5,
        "model_used": "fallback",
        "citations": [],
        "source": {"error": "No suggestions generated"}
    }
